{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = x_train.reshape(-1, 28*28)\n",
    "X_test = x_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "Y_train = encoder.transform(y_train)\n",
    "Y_train = np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit(y_test)\n",
    "Y_test = encoder.transform(y_test)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "Y_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0wnd/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_val = normalize(X_val)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_init_name = 'glorot_normal'  # glorot_normal : Xavier, he_normal : He\n",
    "a_func_name = 'relu'\n",
    "d_rate = 0.3\n",
    "\n",
    "def M_classification():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=X_train.shape[-1], kernel_initializer=w_init_name, bias_initializer='zeros', activation=a_func_name))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(d_rate))\n",
    "    \n",
    "    model.add(Dense(50, activation=a_func_name))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(d_rate))\n",
    "    \n",
    "    model.add(Dense(50, activation=a_func_name))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(d_rate))\n",
    "    \n",
    "    model.add(Dense(np.unique(y_train).size, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Model = KerasClassifier(M_classification, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 1s - loss: 1.6578 - acc: 0.4469     \n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.8850 - acc: 0.7129     \n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.6392 - acc: 0.7986     \n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.5379 - acc: 0.8365     \n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.4738 - acc: 0.8597     \n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.4163 - acc: 0.8771     \n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3863 - acc: 0.8835     \n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3513 - acc: 0.8953     \n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3295 - acc: 0.9033     \n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3125 - acc: 0.9086     \n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2984 - acc: 0.9115     \n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2819 - acc: 0.9164     \n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2592 - acc: 0.9224     \n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2425 - acc: 0.9240     \n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2244 - acc: 0.9318     \n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 1s - loss: 0.2292 - acc: 0.9331     \n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2227 - acc: 0.9343     \n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2061 - acc: 0.9384     \n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2124 - acc: 0.9374     \n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2097 - acc: 0.9353     \n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1992 - acc: 0.9420     \n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1889 - acc: 0.9450     \n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1842 - acc: 0.9458     \n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1659 - acc: 0.9505     \n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1674 - acc: 0.9479     \n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1594 - acc: 0.9544     \n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1735 - acc: 0.9474     \n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1646 - acc: 0.9497     \n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1697 - acc: 0.9491     \n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1518 - acc: 0.9537     \n",
      "2800/4000 [====================>.........] - ETA: 0sEpoch 1/30\n",
      "8000/8000 [==============================] - 1s - loss: 1.6316 - acc: 0.4710     \n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.8495 - acc: 0.735 - 1s - loss: 0.8464 - acc: 0.7356     \n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.6442 - acc: 0.8001     \n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.5382 - acc: 0.8410     \n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.4582 - acc: 0.8614     \n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.4266 - acc: 0.8711     \n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3905 - acc: 0.8847     \n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 1s - loss: 0.3499 - acc: 0.8960     \n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3379 - acc: 0.9003     \n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3269 - acc: 0.9023     \n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2989 - acc: 0.9096     \n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2694 - acc: 0.9209     \n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2592 - acc: 0.9228     \n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2485 - acc: 0.9233     \n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2441 - acc: 0.9245     \n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2224 - acc: 0.9311     \n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2298 - acc: 0.9290     \n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2206 - acc: 0.9348     \n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2079 - acc: 0.9360     \n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2188 - acc: 0.9340     \n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2035 - acc: 0.9396     \n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1978 - acc: 0.9410     \n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1870 - acc: 0.9458     \n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1744 - acc: 0.9475     \n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1754 - acc: 0.9486     \n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1689 - acc: 0.9506     \n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1666 - acc: 0.9475     \n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1675 - acc: 0.9486     \n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 1s - loss: 0.1667 - acc: 0.9478     \n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1535 - acc: 0.9535     \n",
      "3300/4000 [=======================>......] - ETA: 0sEpoch 1/30\n",
      "8000/8000 [==============================] - 1s - loss: 1.6818 - acc: 0.4464     \n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.8876 - acc: 0.7218     \n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.6727 - acc: 0.7952     \n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.5565 - acc: 0.8309     \n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.4650 - acc: 0.8584     \n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.4280 - acc: 0.8635     \n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3886 - acc: 0.8871     \n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3534 - acc: 0.8879     \n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3372 - acc: 0.8969     \n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3155 - acc: 0.9064     \n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.3024 - acc: 0.9094     \n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2846 - acc: 0.9139     \n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2768 - acc: 0.9169     \n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2528 - acc: 0.9211     \n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2469 - acc: 0.9246     \n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2309 - acc: 0.9270     \n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2251 - acc: 0.9328     \n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2190 - acc: 0.9331     \n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2254 - acc: 0.9298     \n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2071 - acc: 0.9375     \n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.2027 - acc: 0.9358     \n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1985 - acc: 0.9410     \n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1865 - acc: 0.9386     \n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1870 - acc: 0.9424     \n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1732 - acc: 0.9454     \n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 0.1663 - acc: 0.9494     \n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1643 - acc: 0.9491     \n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1604 - acc: 0.9497     \n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1550 - acc: 0.9521     \n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 0s - loss: 0.1557 - acc: 0.9539     \n",
      "3400/4000 [========================>.....] - ETA: 0sEpoch 1/50\n",
      "8000/8000 [==============================] - 1s - loss: 1.7478 - acc: 0.4209     \n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.9081 - acc: 0.7100     \n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.6924 - acc: 0.7772     \n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.5706 - acc: 0.8211     \n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.5019 - acc: 0.8499     \n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.4656 - acc: 0.8600     \n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.4221 - acc: 0.8705     \n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.4063 - acc: 0.8790     \n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3631 - acc: 0.8931     \n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3480 - acc: 0.8966     \n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3317 - acc: 0.8994     \n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3006 - acc: 0.9085     \n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2966 - acc: 0.9134     \n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2881 - acc: 0.9140     \n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2626 - acc: 0.9186     \n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2621 - acc: 0.9198     \n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2412 - acc: 0.9255     \n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2404 - acc: 0.9296     \n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2303 - acc: 0.9285     \n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2191 - acc: 0.9353     \n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2192 - acc: 0.9339     \n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1952 - acc: 0.9409     \n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2104 - acc: 0.9339     \n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1960 - acc: 0.9403     \n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1974 - acc: 0.9374     \n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1892 - acc: 0.9421     \n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1825 - acc: 0.9436     \n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1775 - acc: 0.9446     \n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1747 - acc: 0.9492     \n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1659 - acc: 0.9485     \n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1594 - acc: 0.9515     \n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1610 - acc: 0.9500     \n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1615 - acc: 0.9521     \n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1629 - acc: 0.9510     \n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1454 - acc: 0.9530     \n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1569 - acc: 0.9509     \n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1448 - acc: 0.9544     \n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1323 - acc: 0.9596     \n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1410 - acc: 0.9571     \n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1382 - acc: 0.9546     \n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1457 - acc: 0.9555     \n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1312 - acc: 0.9601     \n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1463 - acc: 0.9544     \n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1296 - acc: 0.9596     \n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1306 - acc: 0.9575     \n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1299 - acc: 0.9588     \n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1238 - acc: 0.9626     \n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1217 - acc: 0.9624     \n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1199 - acc: 0.9641     \n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1110 - acc: 0.9625     \n",
      "2900/4000 [====================>.........] - ETA: 0sEpoch 1/50\n",
      "8000/8000 [==============================] - 1s - loss: 1.5751 - acc: 0.4827     \n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.8469 - acc: 0.7317     \n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.6327 - acc: 0.8066     \n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.5330 - acc: 0.8371     \n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.4540 - acc: 0.8641     \n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.4037 - acc: 0.8795     \n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3911 - acc: 0.8860     \n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3491 - acc: 0.8975     \n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3303 - acc: 0.9002     \n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3124 - acc: 0.9058     \n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2766 - acc: 0.9169     \n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2769 - acc: 0.9156     \n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2696 - acc: 0.9211     \n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2576 - acc: 0.9231     \n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2480 - acc: 0.9269     \n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2278 - acc: 0.9301     \n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2301 - acc: 0.9316     \n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2169 - acc: 0.9363     \n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2089 - acc: 0.9368     \n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2042 - acc: 0.9363     \n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2092 - acc: 0.9393     \n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1902 - acc: 0.9431     \n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1866 - acc: 0.9450     \n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1784 - acc: 0.9445     \n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1731 - acc: 0.9455     \n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1719 - acc: 0.9470     \n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1566 - acc: 0.9519     \n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1619 - acc: 0.9496     \n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1622 - acc: 0.9511     \n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1516 - acc: 0.9554     \n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1511 - acc: 0.9524     \n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 0.1539 - acc: 0.9531     \n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1455 - acc: 0.9540     \n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1350 - acc: 0.9583     \n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1291 - acc: 0.9603     \n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1288 - acc: 0.9605     \n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1304 - acc: 0.9618     \n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1289 - acc: 0.9608     \n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1362 - acc: 0.9564     \n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1234 - acc: 0.9591     \n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1185 - acc: 0.9639     \n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1224 - acc: 0.9639     \n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1099 - acc: 0.9670     \n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1149 - acc: 0.9636     \n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1147 - acc: 0.9630     \n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1115 - acc: 0.9663     \n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1252 - acc: 0.9600     \n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1109 - acc: 0.9646     \n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1072 - acc: 0.9675     \n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1053 - acc: 0.9673     \n",
      "3600/4000 [==========================>...] - ETA: 0sEpoch 1/50\n",
      "8000/8000 [==============================] - 1s - loss: 1.6509 - acc: 0.4603     \n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.8846 - acc: 0.7174     \n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.6790 - acc: 0.7879     \n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.5602 - acc: 0.8311     \n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.4870 - acc: 0.8550     \n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.4337 - acc: 0.8726     \n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.3975 - acc: 0.8800     \n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3786 - acc: 0.8889     \n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3365 - acc: 0.8985     \n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3047 - acc: 0.9068     \n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.3011 - acc: 0.9071     \n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2796 - acc: 0.9166     \n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2722 - acc: 0.9199     \n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2503 - acc: 0.9230     \n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2340 - acc: 0.9276     \n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2405 - acc: 0.9270     \n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.2264 - acc: 0.9300     \n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2264 - acc: 0.9311     \n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2010 - acc: 0.9383     \n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.2086 - acc: 0.9379     \n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1905 - acc: 0.9434     \n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1860 - acc: 0.9463     \n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1981 - acc: 0.9419     \n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1839 - acc: 0.9421     \n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1878 - acc: 0.9413     \n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1760 - acc: 0.9491     \n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1565 - acc: 0.9519     \n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1589 - acc: 0.9493     \n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1599 - acc: 0.9528     \n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1572 - acc: 0.9519     \n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1481 - acc: 0.9521     \n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1471 - acc: 0.9566     \n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1409 - acc: 0.9570     \n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1311 - acc: 0.9595     \n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1398 - acc: 0.9598     \n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1265 - acc: 0.9623     \n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1317 - acc: 0.9613     \n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1286 - acc: 0.9586     \n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1269 - acc: 0.9616     \n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1230 - acc: 0.9629     \n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1227 - acc: 0.9650     \n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s - loss: 0.1119 - acc: 0.9651     \n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1266 - acc: 0.9615     \n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1303 - acc: 0.9586     \n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1157 - acc: 0.9648     \n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1029 - acc: 0.9670     \n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1076 - acc: 0.9645     \n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1136 - acc: 0.9629     \n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1014 - acc: 0.9688     \n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 0s - loss: 0.1097 - acc: 0.9673     \n",
      "2900/4000 [====================>.........] - ETA: 0sEpoch 1/50\n",
      "12000/12000 [==============================] - 2s - loss: 1.3301 - acc: 0.5681     \n",
      "Epoch 2/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.6863 - acc: 0.7872     \n",
      "Epoch 3/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.5283 - acc: 0.8401     \n",
      "Epoch 4/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.4447 - acc: 0.8651     \n",
      "Epoch 5/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.3935 - acc: 0.8805     \n",
      "Epoch 6/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.3498 - acc: 0.8953     \n",
      "Epoch 7/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.3309 - acc: 0.9009     \n",
      "Epoch 8/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.3094 - acc: 0.9093     \n",
      "Epoch 9/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2893 - acc: 0.9156     \n",
      "Epoch 10/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2684 - acc: 0.9176     \n",
      "Epoch 11/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2566 - acc: 0.9234     \n",
      "Epoch 12/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2536 - acc: 0.9248     \n",
      "Epoch 13/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2400 - acc: 0.9311     \n",
      "Epoch 14/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2232 - acc: 0.9327     \n",
      "Epoch 15/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2184 - acc: 0.9357     \n",
      "Epoch 16/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2194 - acc: 0.9343     \n",
      "Epoch 17/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.2008 - acc: 0.9393     \n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000/12000 [==============================] - 1s - loss: 0.1967 - acc: 0.9417     \n",
      "Epoch 19/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1867 - acc: 0.9428     \n",
      "Epoch 20/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1784 - acc: 0.9448     \n",
      "Epoch 21/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1829 - acc: 0.9443     \n",
      "Epoch 22/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1806 - acc: 0.9467     \n",
      "Epoch 23/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1677 - acc: 0.9504     \n",
      "Epoch 24/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1676 - acc: 0.9495     \n",
      "Epoch 25/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1599 - acc: 0.9521     \n",
      "Epoch 26/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1459 - acc: 0.9570     \n",
      "Epoch 27/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1583 - acc: 0.9530     \n",
      "Epoch 28/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1537 - acc: 0.9526     \n",
      "Epoch 29/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1505 - acc: 0.9523     \n",
      "Epoch 30/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1437 - acc: 0.9565     \n",
      "Epoch 31/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1392 - acc: 0.9577     \n",
      "Epoch 32/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1376 - acc: 0.9590     \n",
      "Epoch 33/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1400 - acc: 0.9579     \n",
      "Epoch 34/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1381 - acc: 0.9588     \n",
      "Epoch 35/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1361 - acc: 0.9596     \n",
      "Epoch 36/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1266 - acc: 0.9609     \n",
      "Epoch 37/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1330 - acc: 0.9579     \n",
      "Epoch 38/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1273 - acc: 0.9612     \n",
      "Epoch 39/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1338 - acc: 0.9593     \n",
      "Epoch 40/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1291 - acc: 0.9611     \n",
      "Epoch 41/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1146 - acc: 0.9658     \n",
      "Epoch 42/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1213 - acc: 0.9642     \n",
      "Epoch 43/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1169 - acc: 0.9622     \n",
      "Epoch 44/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1168 - acc: 0.9651     \n",
      "Epoch 45/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1153 - acc: 0.9631     \n",
      "Epoch 46/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1065 - acc: 0.9667     \n",
      "Epoch 47/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1135 - acc: 0.9646     \n",
      "Epoch 48/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1178 - acc: 0.9625     \n",
      "Epoch 49/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1095 - acc: 0.9643     \n",
      "Epoch 50/50\n",
      "12000/12000 [==============================] - 1s - loss: 0.1161 - acc: 0.9644     \n"
     ]
    }
   ],
   "source": [
    "validator = GridSearchCV(Model, param_grid= {'epochs':[30, 50]})\n",
    "model_hist = validator.fit(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47648/48000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "best_model = validator.best_estimator_.model\n",
    "metric_names = best_model.metrics_names\n",
    "metric_values = best_model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.3064 - acc: 0.9137    \n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.2586 - acc: 0.9270    \n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.2445 - acc: 0.9308    \n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.2320 - acc: 0.9331    \n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.2226 - acc: 0.9371    \n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.2223 - acc: 0.9366    \n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.2122 - acc: 0.9386    \n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.2037 - acc: 0.9426    \n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 15s - loss: 0.1995 - acc: 0.9429    \n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 14s - loss: 0.1920 - acc: 0.9443    \n"
     ]
    }
   ],
   "source": [
    "xavier_hist = best_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.38249847e-05,   2.51805013e-05,   8.43207454e-05, ...,\n",
       "          9.99380946e-01,   1.63713157e-05,   1.48970095e-04],\n",
       "       [  7.07130384e-05,   6.62032631e-04,   9.96821165e-01, ...,\n",
       "          1.73915992e-04,   5.83072833e-04,   1.57260693e-05],\n",
       "       [  3.28999795e-06,   9.99782264e-01,   1.35250966e-05, ...,\n",
       "          4.93305160e-05,   1.00640289e-04,   5.59110276e-06],\n",
       "       ..., \n",
       "       [  7.09620849e-07,   1.23404789e-05,   6.41026327e-06, ...,\n",
       "          1.43513616e-05,   9.02683405e-06,   1.30309185e-04],\n",
       "       [  7.79621629e-08,   3.21746757e-06,   1.85946362e-06, ...,\n",
       "          2.47321191e-06,   6.01682186e-05,   2.94320012e-06],\n",
       "       [  2.05816377e-05,   5.84464715e-06,   1.75999903e-05, ...,\n",
       "          8.60180194e-07,   3.28468559e-05,   2.14011789e-06]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict = best_model.predict(X_test)\n",
    "Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97289999999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "Y_predict = np.argmax(Y_predict, axis=1)\n",
    "metrics.accuracy_score(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.97      0.97      0.97      1032\n",
      "          3       0.96      0.98      0.97      1010\n",
      "          4       0.98      0.96      0.97       982\n",
      "          5       0.98      0.97      0.97       892\n",
      "          6       0.97      0.98      0.98       958\n",
      "          7       0.97      0.97      0.97      1028\n",
      "          8       0.97      0.97      0.97       974\n",
      "          9       0.97      0.96      0.96      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_test, Y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 972,    0,    1,    0,    0,    1,    3,    1,    2,    0],\n",
       "       [   0, 1121,    2,    3,    0,    0,    3,    1,    5,    0],\n",
       "       [   4,    1,  999,    5,    4,    0,    2,    8,    8,    1],\n",
       "       [   0,    0,    3,  988,    0,    8,    0,    8,    3,    0],\n",
       "       [   1,    0,    5,    0,  939,    0,    8,    3,    2,   24],\n",
       "       [   3,    0,    1,    9,    1,  862,    7,    2,    2,    5],\n",
       "       [   6,    3,    0,    0,    2,    4,  940,    0,    3,    0],\n",
       "       [   2,    7,   13,    2,    2,    0,    0,  998,    1,    3],\n",
       "       [   7,    1,    2,    7,    3,    4,    3,    4,  941,    2],\n",
       "       [   3,    4,    0,   11,   10,    4,    1,    6,    1,  969]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12808f7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xavier_hist.history['acc'], 'b:', label=\"Xavier_relu\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.91374999999999995,\n",
       " 0.92702083333333329,\n",
       " 0.93081250000000004,\n",
       " 0.93308333333333338,\n",
       " 0.93712499999999999,\n",
       " 0.93660416666666668,\n",
       " 0.93856249999999997,\n",
       " 0.94258333333333333,\n",
       " 0.94291666666666663,\n",
       " 0.9443125]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xavier_hist.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
