{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두를 위한 딥러닝 강좌\n",
    "  - ML lab 09-2 : Tensorboard(Neural Net for XOR) : http://bit.ly/2yfiC81\n",
    "\n",
    "### TensorBoard : TF loggin/debuggin tool\n",
    "  - Visualize your TF graph\n",
    "  - Plot quantitative metrics(수치매트릭스)\n",
    "  - Show additional data\n",
    "  - Add tf.name_scope for better graph hierarchy\n",
    "\n",
    "### 5 steps of using TensorBoard\n",
    "  1. From TF graph, decide which tensors you want to log\n",
    "  2. Merge all summaries\n",
    "  3. Create writer and add graph\n",
    "  4. Run summary merge and add_summary\n",
    "  5. Launch Tensorboard(Default: http://localhost:6006)\n",
    "\n",
    "### Multiple runs\n",
    "  - FileWriter에서 하위 경로를 다르게 해서 두개의 log를 남긴다.\n",
    "  - logdir 을 parent directory 로 실행\n",
    "  ```python\n",
    "  writer = tf.summary.FileWriter(\"./logs/xor_logs\")\n",
    "  writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "  ```\n",
    "  $tensorboard -logdir=./logs\n",
    "  \n",
    "### tensorflow summary operations ( http://bit.ly/2yeB0xO )\n",
    "  - Writing Summaries : FileWriter, FileWriterCache\n",
    "  - Summary Ops\n",
    "    * tf.summary.tensor_summary : Outputs a Summary protocol buffer with a serialized tensor\n",
    "    * tf.summary.scalar : Outputs a Summary protocol buffer containing a single scalar value\n",
    "    * tf.summary.histogram : The generated Summary has one summary value containing a histogram for values. Adding a histogram summary makes it possible to visualize your data's distribution in TensorBoard\n",
    "    * tf.summary.audio : Outputs a Summary protocol buffer with audio. The summary has up to max_outputs summary values containing audio\n",
    "    * tf.summary.image : Outputs a Summary protocol buffer with images. The summary has up to max_outputs summary values containing images\n",
    "    * tf.summary.merge : Merges summaries. This op creates a Summary protocol buffer that contains the union of all the values in the input summaries.\n",
    "    * tf.summary.merge_all : Merges all summaries collected in the default graph\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(777) # reproductibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "batch_size = 100\n",
    "tb_summary_dir = './tb/mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "100\n",
      "550\n"
     ]
    }
   ],
   "source": [
    "print(int(mnist.train.num_examples))\n",
    "print(batch_size)\n",
    "print(int(mnist.train.num_examples / batch_size)) # 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "tf.summary.image('input', x_image, 3)  # name : tensor, 3 : interpreted as RGB\n",
    "\n",
    "# dropout_rate\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer1') as scope:\n",
    "    W1 = tf.get_variable(\"W\", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "    \n",
    "    tf.summary.histogram(\"X\", X)\n",
    "    tf.summary.histogram(\"weights\", W1)\n",
    "    tf.summary.histogram(\"bias\", b1)\n",
    "    tf.summary.histogram(\"layer\", L1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer2') as scope:\n",
    "    W2 = tf.get_variable(\"W\", shape=[512, 512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W2)\n",
    "    tf.summary.histogram(\"bias\", b2)\n",
    "    tf.summary.histogram(\"layer\", L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer3') as scope:\n",
    "    W3 = tf.get_variable(\"W\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]))\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W3)\n",
    "    tf.summary.histogram(\"bias\", b3)\n",
    "    tf.summary.histogram(\"layer\", L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer4') as scope:\n",
    "    W4 = tf.get_variable(\"W\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([512]))\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W4)\n",
    "    tf.summary.histogram(\"bias\", b4)\n",
    "    tf.summary.histogram(\"layer\", L4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('layer5') as scope:\n",
    "    W5 = tf.get_variable(\"W\", shape=[512, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "    tf.summary.histogram(\"weights\", W5)\n",
    "    tf.summary.histogram(\"bias\", b5)\n",
    "    tf.summary.histogram(\"hypothesis\", hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "tf.summary.scalar(\"loss\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Create summary writer\n",
    "writer = tf.summary.FileWriter(tb_summary_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning!\n"
     ]
    }
   ],
   "source": [
    "print('Start learning!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.444443378\n",
      "Epoch: 0002 cost = 0.162843547\n",
      "Epoch: 0003 cost = 0.121659727\n",
      "Epoch: 0004 cost = 0.096558064\n",
      "Epoch: 0005 cost = 0.086891630\n",
      "Epoch: 0006 cost = 0.075660631\n",
      "Epoch: 0007 cost = 0.067581554\n",
      "Epoch: 0008 cost = 0.062973251\n",
      "Epoch: 0009 cost = 0.055463182\n",
      "Epoch: 0010 cost = 0.053014139\n",
      "Epoch: 0011 cost = 0.049785875\n",
      "Epoch: 0012 cost = 0.047127293\n",
      "Epoch: 0013 cost = 0.048205956\n",
      "Epoch: 0014 cost = 0.043135369\n",
      "Epoch: 0015 cost = 0.040059927\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "        writer.add_summary(s, global_step=global_step)\n",
    "        global_step += 1\n",
    "\n",
    "        avg_cost += sess.run(cost, feed_dict=feed_dict) / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9832\n"
     ]
    }
   ],
   "source": [
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [4]\n",
      "Prediction:  [4]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADP9JREFUeJzt3W+IXfWdx/HPZ237xDagzd0QrDqJyIIENoVLUqxIS/8k\n1ULsk7EJxCzITtFuaaGIjnlQQcKETf/QB01luoam2p2m0Ip54B80FKS4FEdx/VPX1Z1MaEL+TLAQ\n+6jVfvtgjt1JnHvu9Z5z77mZ7/sFw9x7vufc8/XiJ+fe8ztzfo4IAcjnH5puAEAzCD+QFOEHkiL8\nQFKEH0iK8ANJEX4gKcIPJEX4gaQ+NMydrV69OsbGxoa5SyCV+fl5nT171r2sWyn8trdK+qGkSyT9\nR0TsLVt/bGxMs7OzVXYJoES73e553b4/9tu+RNKPJH1J0nWSttu+rt/XAzBcVb7zb5L0ZkTMRcSf\nJf1C0rZ62gIwaFXCf4WkPyx5frxYdh7bE7Znbc8uLCxU2B2AOg38bH9ETEdEOyLarVZr0LsD0KMq\n4T8h6colzz9RLANwEagS/uckXWt7ne2PSPqqpMP1tAVg0Poe6ouId2z/m6QntTjUdyAiXq2tMwAD\nVWmcPyIek/RYTb0AGCIu7wWSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivAD\nSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gqaFO0Q3Uae/e0kmhNT093bE2NzdXdzsX\nHY78QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5BUpXF+2/OS3pb0rqR3IqJdR1OAJM3MzJTWJycnS+tT\nU1N1trPi1HGRz2cj4mwNrwNgiPjYDyRVNfwh6Wnbz9ueqKMhAMNR9WP/DRFxwvY/SnrK9v9ExDNL\nVyj+UZiQpKuuuqri7gDUpdKRPyJOFL/PSHpE0qZl1pmOiHZEtFutVpXdAahR3+G3fantj733WNIX\nJb1SV2MABqvKx/41kh6x/d7r/GdEPFFLVwAGru/wR8ScpH+usRfgPLt37660/Y033lhTJysTQ31A\nUoQfSIrwA0kRfiApwg8kRfiBpLh1Nxrz7LPPltaPHj1aWl+3bl1pfcOGDR+4p0w48gNJEX4gKcIP\nJEX4gaQIP5AU4QeSIvxAUozzozEPPfRQpe23bNlSWl+1alWl11/pOPIDSRF+ICnCDyRF+IGkCD+Q\nFOEHkiL8QFKM84+Ac+fOldbvvvvu0vpdd93VsbZ+/fq+ehqGJ598stL23Jq7Go78QFKEH0iK8ANJ\nEX4gKcIPJEX4gaQIP5BU13F+2wckfVnSmYjYUCy7XNIhSWOS5iWNR8QfB9fmyrZ///7S+gMPPFBa\n37lzZ8da0+P8c3NzHWvd7svfzebNmyttn10vR/6fStp6wbJ7JB2JiGslHSmeA7iIdA1/RDwj6a0L\nFm+TdLB4fFDSLTX3BWDA+v3OvyYiThaPT0laU1M/AIak8gm/iAhJ0alue8L2rO3ZhYWFqrsDUJN+\nw3/a9lpJKn6f6bRiRExHRDsi2q1Wq8/dAahbv+E/LGlX8XiXpEfraQfAsHQNv+0ZSf8l6Z9sH7d9\nu6S9kr5g+w1Jny+eA7iIdB3nj4jtHUqfq7mXFWtmZqa0Pjk5Wen1r7/++krbD9K+ffv63nbr1gtH\nmM/X9DUMFzuu8AOSIvxAUoQfSIrwA0kRfiApwg8kxa27h2DHjh2Vtp+amqqpk+Hr9ufIZW677bYa\nO8GFOPIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKM89dg795qtzNYt25daX18fLzS6w/SHXfc0fe2\n3f67b7755r5fG91x5AeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpBjnr0HVW2/v2bOntD7Kt6iu8vf6\nExMTpfVVq1b1/drojiM/kBThB5Ii/EBShB9IivADSRF+ICnCDyTVdZzf9gFJX5Z0JiI2FMvuk/Sv\nkhaK1e6NiMcG1eQomJuba7qFRlS9V0GZUb5PQQa9HPl/Kmm5idJ/EBEbi58VHXxgJeoa/oh4RtJb\nQ+gFwBBV+c7/Ddsv2T5g+7LaOgIwFP2G/8eS1kvaKOmkpO91WtH2hO1Z27MLCwudVgMwZH2FPyJO\nR8S7EfFXST+RtKlk3emIaEdEu9Vq9dsngJr1FX7ba5c8/YqkV+ppB8Cw9DLUNyPpM5JW2z4u6TuS\nPmN7o6SQNC/pawPsEcAAdA1/RGxfZvGDA+hlpK1evbpjbevW5UZC/98TTzxRWt+9e3dpffPmzaX1\nQf69/7Fjxwb22qdOnSqtj/J9DFYCrvADkiL8QFKEH0iK8ANJEX4gKcIPJMWtu3tUdhvpQ4cOlW57\n6623lta7DQVec801pfWyqa63bNlSum03VW7N3c39999fWn/88ccHtm9w5AfSIvxAUoQfSIrwA0kR\nfiApwg8kRfiBpBjnr0G3qaS7jVfPzMyU1nfs2FFaP3r0aMfaIMfpq3r99ddL6+fOnSutM4V3NRz5\ngaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApxvlHwPbty90dvfd62fTh+/btK9226nUAZfcSkKQ9e/Z0\nrHW7JTnj+IPFkR9IivADSRF+ICnCDyRF+IGkCD+QFOEHkuo6zm/7Skk/k7RGUkiajogf2r5c0iFJ\nY5LmJY1HxB8H1yo6KZvK+uqrrx7ovsuuMcBo6+XI/46kb0fEdZI+Jenrtq+TdI+kIxFxraQjxXMA\nF4mu4Y+IkxHxQvH4bUmvSbpC0jZJB4vVDkq6ZVBNAqjfB/rOb3tM0icl/U7Smog4WZROafFrAYCL\nRM/ht/1RSb+S9K2IOO/mahERWjwfsNx2E7Znbc8uLCxUahZAfXoKv+0PazH4P4+IXxeLT9teW9TX\nSjqz3LYRMR0R7Yhot1qtOnoGUIOu4bdtSQ9Kei0ivr+kdFjSruLxLkmP1t8egEHp5U96Py1pp6SX\nbb9YLLtX0l5Jv7R9u6RjksYH0yK6KbvF9fT0dKXXnpqaqrQ9RlfX8EfEbyW5Q/lz9bYDYFi4wg9I\nivADSRF+ICnCDyRF+IGkCD+QFLfuXgH279/fsVY2fbfU/dbbd955Z189YfRx5AeSIvxAUoQfSIrw\nA0kRfiApwg8kRfiBpBjnXwHGxzvfSmFycrJ024cffri0zjTZKxdHfiApwg8kRfiBpAg/kBThB5Ii\n/EBShB9IinH+FaBsiu7FmdSA9+PIDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJdQ2/7Stt/8b2722/\navubxfL7bJ+w/WLxc9Pg2wVQl14u8nlH0rcj4gXbH5P0vO2nitoPIuK7g2sPwKB0DX9EnJR0snj8\ntu3XJF0x6MYADNYH+s5ve0zSJyX9rlj0Ddsv2T5g+7IO20zYnrU9u7CwUKlZAPXpOfy2PyrpV5K+\nFRHnJP1Y0npJG7X4yeB7y20XEdMR0Y6IdqvVqqFlAHXoKfy2P6zF4P88In4tSRFxOiLejYi/SvqJ\npE2DaxNA3Xo5229JD0p6LSK+v2T52iWrfUXSK/W3B2BQejnb/2lJOyW9bPvFYtm9krbb3igpJM1L\n+tpAOgQwEL2c7f+tJC9Teqz+dgAMC1f4AUkRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+\nICnCDyRF+IGkCD+QFOEHkvIwp3C2vSDp2JJFqyWdHVoDH8yo9jaqfUn01q86e7s6Inq6X95Qw/++\nnduzEdFurIESo9rbqPYl0Vu/muqNj/1AUoQfSKrp8E83vP8yo9rbqPYl0Vu/Gumt0e/8AJrT9JEf\nQEMaCb/trbZft/2m7Xua6KET2/O2Xy5mHp5tuJcDts/YfmXJssttP2X7jeL3stOkNdTbSMzcXDKz\ndKPv3ajNeD30j/22L5H0v5K+IOm4pOckbY+I3w+1kQ5sz0tqR0TjY8K2b5T0J0k/i4gNxbJ/l/RW\nROwt/uG8LCLuHpHe7pP0p6Znbi4mlFm7dGZpSbdI+hc1+N6V9DWuBt63Jo78myS9GRFzEfFnSb+Q\ntK2BPkZeRDwj6a0LFm+TdLB4fFCL//MMXYfeRkJEnIyIF4rHb0t6b2bpRt+7kr4a0UT4r5D0hyXP\nj2u0pvwOSU/bft72RNPNLGNNMW26JJ2StKbJZpbRdebmYbpgZumRee/6mfG6bpzwe78bImKjpC9J\n+nrx8XYkxeJ3tlEarulp5uZhWWZm6b9r8r3rd8brujUR/hOSrlzy/BPFspEQESeK32ckPaLRm334\n9HuTpBa/zzTcz9+N0szNy80srRF470Zpxusmwv+cpGttr7P9EUlflXS4gT7ex/alxYkY2b5U0hc1\nerMPH5a0q3i8S9KjDfZynlGZubnTzNJq+L0buRmvI2LoP5Ju0uIZ//+TtLuJHjr0tV7Sfxc/rzbd\nm6QZLX4M/IsWz43cLunjko5IekPS05IuH6HeHpL0sqSXtBi0tQ31doMWP9K/JOnF4uempt+7kr4a\ned+4wg9IihN+QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeS+hsSeflwUtFNsgAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1234f7d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = np.random.normal(10, 1, 100) # 평균이 10이고 표준편차가 1인 100개의 랜덤 변수\n",
    "alpha = tf.constant(0.05)               # 가중치 0.05\n",
    "curr_value = tf.placeholder(tf.float32)\n",
    "prev_avg = tf.Variable(0.)\n",
    "update_avg = alpha * curr_value + (1-alpha) * prev_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_hist = tf.summary.scalar(\"running_average\", update_avg)\n",
    "value_hist = tf.summary.scalar(\"incoming_values\", curr_value)\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(len(raw_data)):\n",
    "        summary_str, curr_avg = sess.run([summary, update_avg], feed_dict={curr_value:raw_data[i]})\n",
    "        sess.run(tf.assign(prev_avg, curr_avg))\n",
    "        writer.add_summary(summary_str, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "$ tensorboard --logdir=./logs\n",
    "at browser\n",
    "http://localhost:6006\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
